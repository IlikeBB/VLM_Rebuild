{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d8ada9df9342a0bdd9593850b371dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,262,976 || all params: 8,057,524,224 || trainable%: 0.3384\n",
      "Position interpolate from 16x16 to 32x32\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import signal\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "from main import Main__\n",
    "\n",
    "model_config = {'amp':True, 'use_distributed':False,'accum_grad_iters':1,\n",
    "                'chat_template': True, 'end_sym': '\\n', 'prompt_template': \"<s>[INST] {} [/INST]\",\n",
    "                'max_txt_len': 1024, 'max_context_len': 3500,\n",
    "                'ouput_dir': './exper01_llama2',\n",
    "                'stage_ckpt': '/ssd3/chih/LLM/MiniGPT-4-ckpt/checkpoint_stage3.pth', \n",
    "                'vis_root_train': './dataset/minigpt_casing_train/coco/image/train',\n",
    "                'ann_paths_train': ['./dataset/minigpt_casing_train/coco_caption/defe_ready_anno.json'],\n",
    "                'vis_root_valid': './dataset/minigpt_casing_test/coco/image/test',\n",
    "                'ann_paths_valid': ['./dataset/minigpt_casing_test/coco_caption/defe_ready_anno.json']}\n",
    "\n",
    "# llm_config = {'llama_model':'/ssd3/chih/LLM/Llama-2-7b-chat-hf', 'low_resource':False, 'low_res_device':0, \n",
    "#               'lora_r':64, 'lora_target_modules':[\"q_proj\", \"v_proj\"], 'lora_alpha':16,'lora_dropout':0.05\n",
    "#               }\n",
    "llm_config = {'llama_model':'/ssd3/chih/LLM/Meta-Llama-3-8B-Instruct', 'low_resource':True, 'low_res_device':0, \n",
    "              'lora_r':64, 'lora_target_modules':[\"q_proj\", \"v_proj\"], 'lora_alpha':16,'lora_dropout':0.05\n",
    "              }\n",
    "# '/ssd3/chih/LLM/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "\n",
    "vit_config = {'model_name':'eva_clip_g', 'image_size': 448,  'drop_path_rate': 0, 'use_grad_checkpoint': True, 'vit_precision': 'fp16', 'freeze_vit': True, }\n",
    "\n",
    "lr_config = {'init_lr': 1e-5, 'beta2':0.999,'min_lr': 1e-6, 'decay_rate': None, 'weight_decay':0.05,\n",
    "                'warmup_start_lr': 1e-6, 'warmup_steps': 1000, 'iters_per_epoch': 1000}\n",
    "\n",
    "\n",
    "main_ = Main__(model_config=model_config)\n",
    "main_.VLM_build(llm_config=llm_config, vit_config=vit_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann paths ['./dataset/minigpt_casing_test/coco_caption/defe_ready_anno.json']\n",
      "{'caption': 'Defective.', 'image': '/ssd3/chih/Dataset/minigpt_casing_test/coco/image/test/casting512x512_deffront_0.jpeg', 'image_id': 'casting512x512_deffront_0.jpeg', 'instance_id': '0'}\n",
      "./dataset/minigpt_casing_test/coco/image/test/casting512x512_okfront_1169.jpeg \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.caption_datasets import COCOCaptionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# demo測試對話反饋\n",
    "train_data_set = COCOCaptionDataset(vis_root=model_config['vis_root_valid'], \n",
    "                                ann_paths=model_config['ann_paths_valid'],\n",
    "                                img_size=448)\n",
    "dataloader =DataLoader(train_data_set, batch_size=1, num_workers=1, shuffle=True, pin_memory=True)\n",
    "ckpt_url = '/ssd3/chih/VLM_Rebuild/exper01_llama3/checkpoint_best.pth' \n",
    "main_.load_checkpoint(ckpt_url=ckpt_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "讀取使用fine tune後的llama3權重直接輸出 [同資料集finetune]\n",
      "Q: ['<Img><ImageHere></Img> [caption] Does this photo have any flaws? '] answer: ['yes']\n",
      "A: yes\n"
     ]
    }
   ],
   "source": [
    "samples = next(iter(dataloader))\n",
    "samples['image'] = samples['image'].cuda()\n",
    "ans = main_.chat_module(samples=samples)\n",
    "print(\"讀取使用fine tune後的llama3權重直接輸出 [同資料集finetune]\")\n",
    "print(\"Q:\",samples['instruction_input'], \"answer:\",samples['answer'])\n",
    "print(\"A:\",ans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigptv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
