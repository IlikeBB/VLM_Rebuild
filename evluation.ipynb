{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ec1fca28fe431e92abe8bfe32def78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,262,976 || all params: 8,057,524,224 || trainable%: 0.3384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd3/chih/miniconda3/envs/minigptv/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import signal\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "from main import Main__\n",
    "\n",
    "model_config = {'amp':True, 'use_distributed':False,'accum_grad_iters':1,\n",
    "                'batch_size':4,\n",
    "                'chat_template': True, \n",
    "                'end_sym': [\n",
    "                    '\\n {} </s>',\n",
    "                    \"\\n <|start_header_id|>assistant<|end_header_id|> {} <|eot_id|>\"\n",
    "                ],# list[0] = llama2, list[1] = llama3\n",
    "                'prompt_template': [\n",
    "                \"<s>[INST] {} [/INST]\", \n",
    "                \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|> {} <|eot_id|>\"\"\"\n",
    "                ],# list[0] = llama2, list[1] = llama3\n",
    "                'max_txt_len': 1024, 'max_context_len': 3500,\n",
    "                'ouput_dir': './model_FT_weight/llama3_vit_L-clip336', #./llama3_vit_L-clip336, ./llama3_vit_B-clip224-b16\n",
    "                # 'ouput_dir': './demo', #./llama3_vit_L-clip336, ./llama3_vit_B-clip224-b16\n",
    "                'stage_ckpt': '/ssd3/chih/LLM/MiniGPT-4-ckpt/checkpoint_stage3.pth', \n",
    "                'vis_root_train': './dataset/minigpt_casing_train/coco/image/train',\n",
    "                'ann_paths_train': ['./dataset/minigpt_casing_train/coco_caption/defe_ready_anno.json'],\n",
    "                'vis_root_valid': './dataset/minigpt_casing_test/coco/image/test',\n",
    "                'ann_paths_valid': ['./dataset/minigpt_casing_test/coco_caption/defe_ready_anno.json']}\n",
    "\n",
    "# llm_config = {'llama_model':'/ssd3/chih/LLM/Llama-2-7b-chat-hf', 'low_resource':True, 'low_res_device':0, \n",
    "#               'lora_r':64, 'lora_target_modules':[\"q_proj\", \"v_proj\"], 'lora_alpha':16,'lora_dropout':0.05\n",
    "#               }\n",
    "llm_config = {'llama_model':'/ssd3/chih/LLM/Meta-Llama-3-8B-Instruct', 'low_resource':True, 'low_res_device':0, \n",
    "              'lora_r':64, 'lora_target_modules':[\"q_proj\", \"v_proj\"], 'lora_alpha':16,'lora_dropout':0.05\n",
    "              }\n",
    "# '/ssd3/chih/LLM/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "\n",
    "vit_config = {'model_name':'clip_large_336', #eva_clip_g, clip_large_336\n",
    "            #   'model_path':\"../../VITModel/clip-vit-base-patch16\", #clip-vit-base-patch16, clip-vit-large-patch14-336\n",
    "              'model_path':\"../../VITModel/clip-vit-large-patch14-336\", #clip-vit-base-patch16, clip-vit-large-patch14-336\n",
    "            #   'image_size': 224,  #bilp2 = 448, clip = 224 or 336\n",
    "              'image_size': 336,  #bilp2 = 448, clip = 224 or 336\n",
    "              'drop_path_rate': 0, 'use_grad_checkpoint': True, 'vit_precision': 'fp16', 'freeze_vit': True, }\n",
    "\n",
    "lr_config = {'init_lr': 1e-5, 'beta2':0.999,'min_lr': 1e-6, 'decay_rate': None, 'weight_decay':0.05,\n",
    "                'warmup_start_lr': 1e-6, 'warmup_steps': 1000, 'iters_per_epoch': 1000}\n",
    "\n",
    "\n",
    "main_ = Main__(model_config=model_config)\n",
    "main_.VLM_build(llm_config=llm_config, vit_config=vit_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann paths ['./dataset/minigpt_casing_train/coco_caption/defe_ready_anno.json']\n",
      "{'caption': 'Defective.', 'image': '/ssd3/chih/Dataset/minigpt_casing_train/coco/image/train/casting512x512_deffront_0.jpeg', 'image_id': 'casting512x512_deffront_0.jpeg', 'instance_id': '0'}\n",
      "./dataset/minigpt_casing_train/coco/image/train/casting512x512_deffront_129.jpeg \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.caption_datasets import COCOCaptionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# demo測試對話反饋\n",
    "train_data_set = COCOCaptionDataset(vis_root = model_config['vis_root_train'], \n",
    "                                ann_paths = model_config['ann_paths_train'],\n",
    "                                img_size = vit_config['image_size'])\n",
    "dataloader = DataLoader(train_data_set, batch_size=1, num_workers=5, shuffle=True, pin_memory=True)\n",
    "ckpt_url = '/ssd3/chih/branch/VLM_Rebuild/model_FT_weight/llama3_vit_L-clip336/checkpoint_best.pth' \n",
    "main_.load_checkpoint(ckpt_url=ckpt_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_.model.llama_model.generation_config.pad_token_id = main_.model.llama_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# ====================Prompt Format====================\n",
    "patterns = [\"Llama-3\", \"Llama3\", \"llama3\", \"llama-3\"]\n",
    "pattern = re.compile(\"|\".join(patterns), flags=re.IGNORECASE)\n",
    "if pattern.search(main_.llm_config['llama_model']):\n",
    "    main_.prompt_template = main_.prompt_template[1]\n",
    "    main_.end_sym = main_.end_sym[1]\n",
    "else:\n",
    "    main_.prompt_template = main_.prompt_template[0]\n",
    "    main_.end_sym = main_.end_sym[0]\n",
    "# ========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))\n\u001b[1;32m      2\u001b[0m samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 3\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mmain_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m讀取使用fine tune後的llama3權重直接輸出 [同資料集finetune]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ:\u001b[39m\u001b[38;5;124m\"\u001b[39m,samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction_input\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m,samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/ssd3/chih/branch/VLM_Rebuild/main.py:54\u001b[0m, in \u001b[0;36mMain__.chat_module\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     52\u001b[0m atts_img, img_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_img(samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     53\u001b[0m image_lists \u001b[38;5;241m=\u001b[39m [[image_emb[\u001b[38;5;28;01mNone\u001b[39;00m]] \u001b[38;5;28;01mfor\u001b[39;00m image_emb \u001b[38;5;129;01min\u001b[39;00m img_embeds]\n\u001b[0;32m---> 54\u001b[0m samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction_input\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_template\u001b[38;5;241m.\u001b[39mformat(instruct) \u001b[38;5;28;01mfor\u001b[39;00m instruct \u001b[38;5;129;01min\u001b[39;00m samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction_input\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     55\u001b[0m batch_embs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_context_emb(text, img_list) \u001b[38;5;28;01mfor\u001b[39;00m text, img_list \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction_input\u001b[39m\u001b[38;5;124m'\u001b[39m], image_lists)]\n\u001b[1;32m     56\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_embs)\n",
      "File \u001b[0;32m/ssd3/chih/branch/VLM_Rebuild/main.py:54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m atts_img, img_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_img(samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     53\u001b[0m image_lists \u001b[38;5;241m=\u001b[39m [[image_emb[\u001b[38;5;28;01mNone\u001b[39;00m]] \u001b[38;5;28;01mfor\u001b[39;00m image_emb \u001b[38;5;129;01min\u001b[39;00m img_embeds]\n\u001b[0;32m---> 54\u001b[0m samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction_input\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m(instruct) \u001b[38;5;28;01mfor\u001b[39;00m instruct \u001b[38;5;129;01min\u001b[39;00m samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction_input\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     55\u001b[0m batch_embs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_context_emb(text, img_list) \u001b[38;5;28;01mfor\u001b[39;00m text, img_list \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction_input\u001b[39m\u001b[38;5;124m'\u001b[39m], image_lists)]\n\u001b[1;32m     56\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_embs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    "samples = next(iter(dataloader))\n",
    "samples['image'] = samples['image'].cuda()\n",
    "ans = main_.chat_module(samples=samples)\n",
    "print(\"讀取使用fine tune後的llama3權重直接輸出 [同資料集finetune]\")\n",
    "print(\"Q:\",samples['instruction_input'], \"answer:\",samples['answer'])\n",
    "print(\"A:\",ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigptv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
