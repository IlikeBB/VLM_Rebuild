{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57821bcddabb4c3aac4aad773e124ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,262,976 || all params: 8,057,524,224 || trainable%: 0.3384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd3/chih/miniconda3/envs/minigptv/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import signal\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "from main import Main__\n",
    "\n",
    "model_config = {'amp':True, 'use_distributed':False,'accum_grad_iters':1,\n",
    "                'batch_size':1,\n",
    "                'chat_template': True, 'end_sym': '\\n', 'prompt_template': \"[INST] {} [/INST]\",\n",
    "                'max_txt_len': 1024, 'max_context_len': 3500,\n",
    "                'ouput_dir': './llama3_vit_L-clip336', #./llama3_vit_L-clip336, ./llama3_vit_B-clip224-b16\n",
    "                'stage_ckpt': '/ssd3/chih/LLM/MiniGPT-4-ckpt/checkpoint_stage3.pth', \n",
    "                'vis_root_train': './dataset/minigpt_casing_train/coco/image/train',\n",
    "                'ann_paths_train': ['./dataset/minigpt_casing_train/coco_caption/defe_ready_anno.json'],\n",
    "                'vis_root_valid': './dataset/minigpt_casing_test/coco/image/test',\n",
    "                'ann_paths_valid': ['./dataset/minigpt_casing_test/coco_caption/defe_ready_anno.json']}\n",
    "\n",
    "# llm_config = {'llama_model':'/ssd3/chih/LLM/Llama-2-7b-chat-hf', 'low_resource':False, 'low_res_device':0, \n",
    "#               'lora_r':64, 'lora_target_modules':[\"q_proj\", \"v_proj\"], 'lora_alpha':16,'lora_dropout':0.05\n",
    "#               }\n",
    "llm_config = {'llama_model':'/ssd3/chih/LLM/Meta-Llama-3-8B-Instruct', 'low_resource':True, 'low_res_device':0, \n",
    "              'lora_r':64, 'lora_target_modules':[\"q_proj\", \"v_proj\"], 'lora_alpha':16,'lora_dropout':0.05\n",
    "              }\n",
    "# '/ssd3/chih/LLM/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "\n",
    "vit_config = {'model_name':'clip_large_336', #eva_clip_g, clip_large_336\n",
    "              'model_path':\"../../VITModel/clip-vit-large-patch14-336\", #clip-vit-base-patch16, clip-vit-large-patch14-336\n",
    "              'image_size': 336,  #bilp2 = 448, clip = 224 or 336\n",
    "              'drop_path_rate': 0, 'use_grad_checkpoint': True, 'vit_precision': 'fp16', 'freeze_vit': True, }\n",
    "\n",
    "lr_config = {'init_lr': 1e-5, 'beta2':0.999,'min_lr': 1e-6, 'decay_rate': None, 'weight_decay':0.05,\n",
    "                'warmup_start_lr': 1e-6, 'warmup_steps': 1000, 'iters_per_epoch': 1000}\n",
    "\n",
    "\n",
    "main_ = Main__(model_config=model_config)\n",
    "main_.VLM_build(llm_config=llm_config, vit_config=vit_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann paths ['./dataset/minigpt_casing_train/coco_caption/defe_ready_anno.json']\n",
      "{'caption': 'Defective.', 'image': '/ssd3/chih/Dataset/minigpt_casing_train/coco/image/train/casting512x512_deffront_0.jpeg', 'image_id': 'casting512x512_deffront_0.jpeg', 'instance_id': '0'}\n",
      "./dataset/minigpt_casing_train/coco/image/train/casting512x512_deffront_129.jpeg \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.caption_datasets import COCOCaptionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# demo測試對話反饋\n",
    "train_data_set = COCOCaptionDataset(vis_root = model_config['vis_root_train'], \n",
    "                                ann_paths = model_config['ann_paths_train'],\n",
    "                                img_size = vit_config['image_size'])\n",
    "dataloader = DataLoader(train_data_set, batch_size=1, num_workers=5, shuffle=True, pin_memory=True)\n",
    "ckpt_url = '/ssd3/chih/branch/VLM_Rebuild/llama3_vit_L-clip336/checkpoint_best.pth' \n",
    "main_.load_checkpoint(ckpt_url=ckpt_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_.model.llama_model.generation_config.pad_token_id = main_.model.llama_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>100\n",
      "flawless\n",
      "no\n",
      "flawless\n",
      "flawless\n",
      "flawless\n",
      "\n",
      "讀取使用fine tune後的llama3權重直接輸出 [同資料集finetune]\n",
      "Q: ['<Img><ImageHere></Img> [caption] Please answer Defective product or Flawless product. '] answer: ['no']\n",
      "A: ['<|begin_of_text|>100\\nflawless\\nno\\nflawless\\nflawless\\nflawless']\n"
     ]
    }
   ],
   "source": [
    "samples = next(iter(dataloader))\n",
    "samples['image'] = samples['image'].cuda()\n",
    "ans = main_.chat_module(samples=samples)\n",
    "print(\"讀取使用fine tune後的llama3權重直接輸出 [同資料集finetune]\")\n",
    "print(\"Q:\",samples['instruction_input'], \"answer:\",samples['answer'])\n",
    "print(\"A:\",ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigptv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
