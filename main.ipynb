{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69872/3406762564.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import glob, sys, os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from model.Build_model import build_vlm_model, Main_model\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from utils.caption_datasets import COCOCaptionDataset\n",
    "from utils.lr_sched_cls import LinearWarmupCosineLRScheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'amp':True, 'use_distributed':False,'accum_grad_iters':1,\n",
    "                'chat_template': True, 'end_sym': '\\n', 'prompt_template': \"[INST] {} [/INST]\",\n",
    "                'max_txt_len': 1024, 'max_context_len': 3500,\n",
    "                'ouput_dir': './exper01_llama2',\n",
    "                'stage_ckpt': '/ssd3/chih/LLM/MiniGPT-4-ckpt/checkpoint_stage3.pth', \n",
    "                'vis_root_train': './dataset/minigpt_casing_train/coco/image/train',\n",
    "                'ann_paths_train': ['./dataset/minigpt_casing_train/coco_caption/defe_ready_anno.json'],\n",
    "                'vis_root_valid': './dataset/minigpt_casing_test/coco/image/test',\n",
    "                'ann_paths_valid': ['./dataset/minigpt_casing_test/coco_caption/defe_ready_anno.json']}\n",
    "\n",
    "llm_config = {'llama_model':'/ssd3/chih/LLM/Llama-2-7b-chat-hf', 'low_resource':False, 'low_res_device':0, \n",
    "              'lora_r':64, 'lora_target_modules':[\"q_proj\", \"v_proj\"], 'lora_alpha':16,'lora_dropout':0.05\n",
    "              }\n",
    "# llm_config = {'llama_model':'/ssd3/chih/LLM/Meta-Llama-3-8B-Instruct', 'low_resource':False, 'low_res_device':0, \n",
    "#               'lora_r':64, 'lora_target_modules':[\"q_proj\", \"v_proj\"], 'lora_alpha':16,'lora_dropout':0.05\n",
    "#               }\n",
    "# '/ssd3/chih/LLM/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "\n",
    "vit_config = {'model_name':'eva_clip_g', 'image_size': 448,  'drop_path_rate': 0, 'use_grad_checkpoint': True, 'vit_precision': 'fp16', 'freeze_vit': True, }\n",
    "\n",
    "lr_config = {'init_lr': 1e-5, 'beta2':0.999,'min_lr': 1e-6, 'decay_rate': None, 'weight_decay':0.05,\n",
    "                'warmup_start_lr': 1e-6, 'warmup_steps': 1000, 'iters_per_epoch': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main__:\n",
    "    def __init__(self, model_config: dict):\n",
    "        super().__init__()\n",
    "        self.model_config = model_config\n",
    "        self.save_path = os.path.join(os.path.join(self.model_config['ouput_dir']))\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "        self.end_sym = self.model_config['end_sym']\n",
    "        self.max_context_len = self.model_config['max_context_len']\n",
    "        self.max_txt_len = self.model_config['max_txt_len']\n",
    "        self.chat_template = self.model_config['chat_template']\n",
    "        self.prompt_template = self.model_config['prompt_template']\n",
    "        self.distributed_ = self.model_config['use_distributed']\n",
    "        self.scaler = None\n",
    "        self.curr_device = None #從sample定位\n",
    "\n",
    "    def distributed_DDP(self,device_ids=[]):\n",
    "        from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "        self.model = DDP(self.model, device_ids=device_ids, find_unused_parameters=True)\n",
    "        \n",
    "    def get_context_emb(self, prompt, img_list):\n",
    "        device = img_list[0].device\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "        assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "        seg_tokens = [\n",
    "            self.model.llama_tokenizer(\n",
    "                seg, return_tensors=\"pt\", add_special_tokens=i==0).to(device).input_ids # only add bos to the first seg\n",
    "            for i, seg in enumerate(prompt_segs)\n",
    "        ]\n",
    "        seg_embs = [self.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "\n",
    "        mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "        # print(\"mixed_embs-0\", len(mixed_embs))\n",
    "        # print(f\"前後文字與中間影像embedding shape: 前){mixed_embs[0].shape} 中) {mixed_embs[1].shape} 後){mixed_embs[2].shape}\")\n",
    "        mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "        # print(\"前後文字與影像合併後shape\", mixed_embs.shape)\n",
    "        return mixed_embs\n",
    "\n",
    "    def chat_module(self, samples = None): #不太穩定，僅供測試，直接從minigpt搬過來，尚未整理\n",
    "        atts_img, img_embeds = self.encode_img(samples['image'])\n",
    "        image_lists = [[image_emb[None]] for image_emb in img_embeds]\n",
    "        batch_embs = [self.get_context_emb(text, img_list) for text, img_list in zip(samples['instruction_input'], image_lists)]\n",
    "        batch_size = len(batch_embs)\n",
    "        max_len = max([emb.shape[1] for emb in batch_embs])\n",
    "        emb_dim = batch_embs[0].shape[2]\n",
    "        dtype = batch_embs[0].dtype\n",
    "        device = batch_embs[0].device\n",
    "        embs = torch.zeros([batch_size, max_len, emb_dim], dtype=dtype, device=device)\n",
    "        attn_mask = torch.zeros([batch_size, max_len], dtype=torch.int, device=device)\n",
    "        for i, emb in enumerate(batch_embs):\n",
    "            emb_len = emb.shape[1]\n",
    "            embs[i, -emb_len:] = emb[0]\n",
    "            attn_mask[i, -emb_len:] = 1\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            outputs = self.model.llama_model.generate(\n",
    "                inputs_embeds=embs,\n",
    "                attention_mask=attn_mask,\n",
    "                max_new_tokens=1024,\n",
    "                num_beams=1,\n",
    "                length_penalty=1,\n",
    "                temperature=1,\n",
    "                do_sample=False,\n",
    "                min_length=1,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1,\n",
    "            )\n",
    "        answers = []\n",
    "        for output_token in outputs:\n",
    "            if output_token[0] == 0:\n",
    "                output_token = output_token[1:]\n",
    "            output_texts = self.model.llama_tokenizer.decode(output_token, skip_special_tokens=True)\n",
    "        answers.append(output_texts)\n",
    "        return answers\n",
    "    \n",
    "    def concat_emb_input_output(self, input_embs, input_atts, output_embs, output_atts):\n",
    "        \"\"\"\n",
    "        Concatenate the batched input embedding and batched output embedding together.\n",
    "        Both the input and the output embedding should be right padded.\n",
    "        \"\"\"\n",
    "        input_lens = []\n",
    "        cat_embs = []\n",
    "        cat_atts = []\n",
    "        for i in range(input_embs.size(0)):\n",
    "            input_len = input_atts[i].sum()\n",
    "            input_lens.append(input_len)\n",
    "            cat_embs.append(\n",
    "                torch.cat([\n",
    "                    input_embs[i][:input_len],\n",
    "                    output_embs[i],\n",
    "                    input_embs[i][input_len:]\n",
    "                ])\n",
    "            )\n",
    "            cat_atts.append(\n",
    "                torch.cat([\n",
    "                    input_atts[i][:input_len],\n",
    "                    output_atts[i],\n",
    "                    input_atts[i][input_len:]\n",
    "                ])\n",
    "            )\n",
    "        cat_embs = torch.stack(cat_embs)\n",
    "        cat_atts = torch.stack(cat_atts)\n",
    "        return cat_embs, cat_atts, input_lens\n",
    "    \n",
    "    def embed_tokens(self, token_ids):\n",
    "        # print(\"token_ids: \", token_ids)\n",
    "        if hasattr(self.model.llama_model.base_model, 'model'): ## lora wrapped model\n",
    "            embeds = self.model.llama_model.base_model.model.model.embed_tokens(token_ids)\n",
    "        else:\n",
    "            embeds = self.model.llama_model.base_model.embed_tokens(token_ids)\n",
    "        return embeds\n",
    "    \n",
    "    def prompt_wrap(self, img_embeds, atts_img, prompts, lengths=None, tokenizer = None):    \n",
    "        # print(img_embeds.shape, atts_img.shape)\n",
    "        emb_lists = []\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts] * len(img_embeds)\n",
    "        \n",
    "        # 每一個batch一個一個處理\n",
    "        for idx, (each_img_embed, each_prompt) in enumerate(zip(img_embeds, prompts)):\n",
    "            pn = each_img_embed.shape[-2]\n",
    "            p_segs = each_prompt.split('<ImageHere>')\n",
    "            # print(p_segs)\n",
    "            interleave_emb = []\n",
    "            # 前半段句子\n",
    "            for idx, seg in enumerate(p_segs[:-1]): #該loop是為了應用於影片，單張圖像也可使用\n",
    "                p_tokens = tokenizer(seg, return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n",
    "                p_embed = self.embed_tokens(p_tokens.input_ids)\n",
    "                # interleave_emb.append(torch.cat([p_embed, each_img_embed[None][:, idx * pn:(idx + 1) * pn]], dim=1))\n",
    "                interleave_emb.append(torch.cat([p_embed, each_img_embed.unsqueeze(0)], dim=1))\n",
    "            # 前半段句子'<Img>'+圖像'{image embed}'\n",
    "            wrapped_emb = torch.cat(interleave_emb, dim=1)\n",
    "            p_tokens = tokenizer(p_segs[-1], return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n",
    "            p_embed = self.embed_tokens(p_tokens.input_ids)\n",
    "            wrapped_emb = torch.cat([wrapped_emb, p_embed], dim=1)\n",
    "            emb_lists.append(wrapped_emb)\n",
    "        emb_lens = [emb.shape[1] for emb in emb_lists]\n",
    "        pad_emb = self.embed_tokens(torch.tensor(tokenizer.pad_token_id, device=img_embeds.device))\n",
    "        max_length = max(emb_lens) if max(emb_lens) < self.max_context_len else self.max_context_len\n",
    "        wrapped_embs = pad_emb.expand(len(emb_lens), max_length, -1).clone()\n",
    "        wrapped_atts = torch.zeros([len(emb_lens), max_length], dtype=torch.int, device=img_embeds.device)\n",
    "        \n",
    "        # 每一個batch一個一個處理\n",
    "        for i, emb in enumerate(emb_lists):\n",
    "            length = emb_lens[i] if emb_lens[i] < self.max_context_len else self.max_context_len\n",
    "            wrapped_embs[i, :length] = emb[:, :length]\n",
    "            wrapped_atts[i, :length] = 1\n",
    "        return wrapped_embs, wrapped_atts\n",
    "    \n",
    "    def VLM_build(self, llm_config:dict, vit_config:dict):\n",
    "        self.llm_config = llm_config\n",
    "        self.vit_config = vit_config\n",
    "        self.model = Main_model(llm_config=llm_config, vit_config=vit_config)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda()\n",
    "            \n",
    "    def init_optimizer(self, lr_config:dict):\n",
    "        num_parameters = 0\n",
    "        p_wd, p_non_wd = [], []\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            if p.ndim <2 or 'bias' in n or 'ln' in n or 'bn' in n:\n",
    "                p_non_wd.append(p)\n",
    "            else:\n",
    "                p_wd.append(p)\n",
    "            num_parameters+=p.data.nelement()\n",
    "            optim_params = [\n",
    "                {\n",
    "                    \"params\": p_wd,\n",
    "                    \"weight_decay\": lr_config['weight_decay'],\n",
    "                },\n",
    "                {\"params\": p_non_wd, \"weight_decay\": 0},\n",
    "            ]\n",
    "            beta2 = lr_config['beta2']\n",
    "            self.optimizer = torch.optim.AdamW(\n",
    "                optim_params,\n",
    "                lr=float(lr_config['init_lr']),\n",
    "                weight_decay=lr_config['weight_decay'],\n",
    "                betas=(0.9, beta2),\n",
    "            )\n",
    "        return self.optimizer\n",
    "    \n",
    "    def lr_scheduler_cls(self, lr_config: dict):\n",
    "        self._lr_sched = LinearWarmupCosineLRScheduler(\n",
    "            optimizer=self.optimizer,\n",
    "            max_epoch=int(self.max_epoch),\n",
    "            iters_per_epoch=int(lr_config['iters_per_epoch']),\n",
    "            min_lr=lr_config['min_lr'],\n",
    "            init_lr=lr_config['init_lr'],\n",
    "            decay_rate=lr_config['decay_rate'],\n",
    "            warmup_start_lr=lr_config['warmup_start_lr'],\n",
    "            warmup_steps=lr_config['warmup_steps'],\n",
    "        )\n",
    "        return self._lr_sched\n",
    "    \n",
    "    def load_checkpoint(self, ckpt_url: str):  \n",
    "        weight = torch.load(ckpt_url, map_location=\"cpu\")['model']\n",
    "        self.model.load_state_dict(weight, strict=False)\n",
    "        \n",
    "    def _save_checkpoint(self, cur_epoch, is_best=False):\n",
    "        if self.distributed_:\n",
    "            model_no_ddp = self.model.module\n",
    "        else:\n",
    "            model_no_ddp = self.model\n",
    "        param_grad_dic = {\n",
    "            k: v.requires_grad for (k, v) in model_no_ddp.named_parameters()\n",
    "        }\n",
    "        state_dict = model_no_ddp.state_dict() \n",
    "        for k in list(state_dict.keys()):#只保留有做gradient的layer做儲存(e.g: lora、prompt layer)\n",
    "            if k in param_grad_dic.keys() and not param_grad_dic[k]:\n",
    "                del state_dict[k]\n",
    "        save_obj = {\n",
    "            \"model\": state_dict,\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"config\":{\n",
    "                'model_config':self.model_config,\n",
    "                'llm_config':self.llm_config,\n",
    "                'vit_config':self.vit_config\n",
    "                },\n",
    "            \"scaler\": self.scaler.state_dict() if self.scaler else None,\n",
    "            \"epoch\": cur_epoch,\n",
    "        }\n",
    "        save_to = os.path.join(\n",
    "            self.save_path,\n",
    "            \"checkpoint_{}.pth\".format(\"best\" if is_best else cur_epoch),\n",
    "        )\n",
    "        self.best_weight = save_obj\n",
    "        \n",
    "        torch.save(save_obj, save_to)\n",
    "    def encode_img(self, image):\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            image_embeds = self.model.ln_vision(self.model.visual_encoder(image))\n",
    "            image_embeds = image_embeds[:, 1:, :]\n",
    "            bs, pn, hs = image_embeds.shape\n",
    "            image_embeds = image_embeds.view(bs, int(pn / 4), int(hs * 4))\n",
    "            img_embeds = self.model.llama_proj(image_embeds.cuda().to(torch.float32))\n",
    "            img_atts = torch.ones(img_embeds.size()[:-1], dtype=torch.long).to(image.device)\n",
    "            return img_atts, img_embeds\n",
    "            \n",
    "    def train_step(self, model = None, samples = None, p_type = 'train'):\n",
    "        if torch.cuda.is_available():\n",
    "            image = samples[\"image\"].cuda()\n",
    "        else:\n",
    "            image = samples[\"image\"]\n",
    "        self.curr_device = image.device\n",
    "\n",
    "        img_atts, img_embeds = self.encode_img(image)\n",
    "    \n",
    "        # question process\n",
    "        instruction = samples[\"instruction_input\"]\n",
    "        if self.chat_template: #添加[INSt]{}[/INST]\n",
    "                instruction = [self.prompt_template.format(instruct) for instruct in instruction]\n",
    "        cond_embeds, cond_atts = self.prompt_wrap(img_embeds, img_atts, instruction, tokenizer = self.model.llama_tokenizer)\n",
    "\n",
    "        # answer process\n",
    "        self.model.llama_tokenizer.padding_side = 'right'\n",
    "        text = [t + self.end_sym for t in samples[\"answer\"]] # 加上換行結束符號\n",
    "        regress_tokens = self.model.llama_tokenizer(text,\n",
    "                                            return_tensors=\"pt\", padding=\"longest\", truncation=True, \n",
    "                                            max_length=self.max_txt_len, add_special_tokens=False\n",
    "                                            ).to(self.curr_device)\n",
    "        regress_token_ids = regress_tokens.input_ids\n",
    "        regress_atts = regress_tokens.attention_mask\n",
    "        part_targets = regress_token_ids.masked_fill(regress_token_ids == self.model.llama_tokenizer.pad_token_id, -100)\n",
    "        regress_embeds = self.embed_tokens(regress_token_ids)\n",
    "        \n",
    "        # concat the embedding to condition and the embedding to regress\n",
    "        inputs_embeds, attention_mask, input_lens = self.concat_emb_input_output(cond_embeds, cond_atts, regress_embeds, regress_atts)\n",
    "        bos = torch.ones_like(part_targets[:, :1]) * self.model.llama_tokenizer.bos_token_id\n",
    "        bos_embeds = self.embed_tokens(bos)\n",
    "        bos_atts = cond_atts[:, :1]\n",
    "\n",
    "        # add bos token at the begining\n",
    "        inputs_embeds = torch.cat([bos_embeds, inputs_embeds], dim=1)\n",
    "        attention_mask = torch.cat([bos_atts, attention_mask], dim=1)\n",
    "        \n",
    "        # ensemble the final targets\n",
    "        targets = torch.ones([inputs_embeds.shape[0], inputs_embeds.shape[1]],\n",
    "                            dtype=torch.long).to(self.curr_device).fill_(-100)\n",
    "        for i, target in enumerate(part_targets):\n",
    "            targets[i, input_lens[i]+1:input_lens[i]+len(target)+1] = target  # plus 1 for bos\n",
    "            \n",
    "        # input data, get loss\n",
    "        outputs = self.model.llama_model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True,\n",
    "                labels=targets,\n",
    "                reduction='mean'\n",
    "            )\n",
    "        loss = outputs[\"loss\"]    \n",
    "        return loss\n",
    "    \n",
    "    def valid(self, dataloader = None, cur_epoch=None):\n",
    "        valid_loss_stack = []\n",
    "        with torch.no_grad():\n",
    "            if self.distributed_:\n",
    "                model_no_ddp = self.model.module\n",
    "            else:\n",
    "                model_no_ddp = self.model\n",
    "            model_no_ddp.eval()\n",
    "            # for cur_iter in tqdm(range(100)):\n",
    "            for cur_iter in tqdm(range(dataloader.__len__())):\n",
    "                samples = next(iter(dataloader))\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    loss = self.train_step(model=self.model, samples=samples, p_type='valid')\n",
    "                    valid_loss_stack.append(loss.item())\n",
    "            avg_loss = np.average(valid_loss_stack)\n",
    "            print(\"*************************************************************\")\n",
    "            print(f\"[Epoch: {cur_epoch} End]Validation Average loss = {avg_loss}\")\n",
    "            print(\"*************************************************************\")\n",
    "            \n",
    "            if avg_loss<self.best_loss:\n",
    "                self._save_checkpoint(cur_epoch=cur_epoch, is_best=True)\n",
    "                self.best_loss = avg_loss\n",
    "            else:\n",
    "                self.model.load_state_dict(self.best_weight['model'], strict=False)\n",
    "                print(\"*************************************************************\")\n",
    "                print(f\"Loading best epoch from: {self.best_weight['epoch']}!!!!!!!!!!!\")\n",
    "                print(\"*************************************************************\")\n",
    "    def trainer(self, dataloader = None, cur_epoch=None):\n",
    "        train_loss_stack = []\n",
    "        # 採用minigpt，依照設定每一個epoch的iteration的長度建立train process\n",
    "        # 可以方便調整scaler、\n",
    "        self.model.train()\n",
    "        # for samples in dataloader:\n",
    "        for cur_iter in range(self.lr_config['iters_per_epoch']):\n",
    "            # samples = {'image': torch.tensor, 'answer': str, 'instruction_input':str,\n",
    "            #            'epoch': int, 'num_iters_per_epoch':int, 'iters':int}\n",
    "            if cur_iter >= self.lr_config['iters_per_epoch']:\n",
    "                break\n",
    "            samples = next(iter(dataloader))\n",
    "            samples.update(\n",
    "                {\n",
    "                    \"epoch\": cur_epoch,\n",
    "                    \"num_iters_per_epoch\": self.lr_config['iters_per_epoch'],\n",
    "                    \"iters\": cur_iter,\n",
    "                }\n",
    "            )\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                loss = self.train_step(model=self.model, samples=samples)\n",
    "            \n",
    "            if self.model_config['amp']:\n",
    "                self.scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            # update gradients every accum_grad_iters iterations\n",
    "            if (cur_iter+1) % self.model_config['accum_grad_iters'] ==0:\n",
    "                if self.model_config['amp']:\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            train_loss_stack.append(loss.item())\n",
    "            if cur_iter%100==0 and cur_iter!=0:\n",
    "                print(f\"[Epoch: {cur_epoch}][{cur_iter}/{self.lr_config['iters_per_epoch']}]Current Average loss = {np.average(train_loss_stack)}\")\n",
    "            # break\n",
    "        print(f\"[Epoch: {cur_epoch} End]Average loss = {np.average(train_loss_stack)}\")\n",
    "    def main_process(self, max_epoch = 20,lr_config=None):\n",
    "        self.lr_config = lr_config\n",
    "        self.best_agg_metric = 0\n",
    "        self.best_epoch = 0\n",
    "        self.best_loss = np.inf\n",
    "        if self.distributed_:\n",
    "            self.distributed_DDP(device_ids=[0,1])\n",
    "        self.max_epoch = max_epoch\n",
    "        if self.model_config['amp']:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "        self.optimizer = self.init_optimizer(lr_config = self.lr_config)\n",
    "        self.lr_scheduler = self.lr_scheduler_cls(lr_config = self.lr_config)\n",
    "        \n",
    "        train_data_set = COCOCaptionDataset(vis_root=self.model_config['vis_root_train'], \n",
    "                                      ann_paths=self.model_config['ann_paths_train'],\n",
    "                                      img_size = self.vit_config['image_size'])\n",
    "        train_dataloader =DataLoader(train_data_set, batch_size=1, num_workers=1, shuffle=True, pin_memory=True)\n",
    "        \n",
    "        if self.model_config['vis_root_valid']!=None and self.model_config['ann_paths_valid']!=None:\n",
    "            valid_data_set = COCOCaptionDataset(vis_root=self.model_config['vis_root_train'], \n",
    "                                                ann_paths=self.model_config['ann_paths_train'],\n",
    "                                                img_size = self.vit_config['image_size'])\n",
    "            valid_dataloader = DataLoader(valid_data_set, batch_size=1, num_workers=1, shuffle=True, pin_memory=True)\n",
    "        else:\n",
    "            valid_dataloader = DataLoader(train_data_set, batch_size=1, num_workers=1, shuffle=False, pin_memory=True)\n",
    "        \n",
    "        \n",
    "        for cur_epoch in range(1, self.max_epoch):\n",
    "            self.trainer(dataloader=train_dataloader, cur_epoch=cur_epoch)\n",
    "            self.valid(dataloader=valid_dataloader, cur_epoch=cur_epoch)\n",
    "            # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ = Main__(model_config=model_config)\n",
    "main_.VLM_build(llm_config=llm_config, vit_config=vit_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_.main_process(max_epoch=3, lr_config=lr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # demo測試對話反饋\n",
    "# train_data_set = COCOCaptionDataset(vis_root=model_config['vis_root_train'], \n",
    "#                                 ann_paths=model_config['ann_paths_train'])\n",
    "# dataloader =DataLoader(train_data_set, batch_size=1, num_workers=1, shuffle=False, pin_memory=True)\n",
    "# # ckpt_url = '/ssd3/chih/MiniGPT-4_PhisonVer/demo_ckpt/casting_checkpoint_49_137.pth' \n",
    "# # main_.load_checkpoint(ckpt_url=ckpt_url)\n",
    "# samples = next(iter(dataloader))\n",
    "# samples['image'] = samples['image'].cuda()\n",
    "# ans = main_.chat_module(samples=samples)\n",
    "# print(ans)\n",
    "# ['there are defects in the product in the photo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grad_dic = {\n",
    "#     k: v.requires_grad for (k, v) in main_.model.named_parameters()\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann paths ['/ssd3/chih/Dataset/minigpt_casing_test/coco_caption/defe_ready_anno.json']\n",
      "{'caption': 'Defective.', 'image': '/home/deepmentor/acvlab_exper/dataset/minigpt_casing_test/coco/image/test/casting512x512_deffront_0.jpeg', 'image_id': 'casting512x512_deffront_0.jpeg', 'instance_id': '0'}\n",
      "/ssd3/chih/Dataset/minigpt_casing_test/coco/image/test/casting512x512_okfront_1169.jpeg \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 448, 448])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_set = COCOCaptionDataset(vis_root=model_config['vis_root_valid'], \n",
    "                                    ann_paths=model_config['ann_paths_valid'],\n",
    "                                    img_size = vit_config['image_size'])\n",
    "\n",
    "for i in valid_data_set:\n",
    "    break\n",
    "i['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigptv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
