{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minigpt Vit module\n",
    "#   (visual_encoder): VisionTransformer(\n",
    "#     (patch_embed): PatchEmbed(\n",
    "#       (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
    "#     )\n",
    "#     (pos_drop): Dropout(p=0.0, inplace=False)\n",
    "#     (blocks): ModuleList(\n",
    "#       (0-38): 39 x Block(\n",
    "#         (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
    "#         (attn): Attention(\n",
    "#           (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
    "#           (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "#           (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
    "#           (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "#         )\n",
    "#         (drop_path): Identity()\n",
    "#         (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
    "#         (mlp): Mlp(\n",
    "#           (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
    "#           (act): GELU(approximate='none')\n",
    "#           (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
    "#           (drop): Dropout(p=0.0, inplace=False)\n",
    "#         )\n",
    "#       )\n",
    "#     )\n",
    "#   )\n",
    "#   (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
    "#   (llama_proj): Linear(in_features=5632, out_features=4096, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os,sys\n",
    "import torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from model.eva_vit import create_eva_vit_g,VisionTransformer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "# processor = BlipProcessor.from_pretrained(\"../../VITModel/blip-vqa-base/\")\n",
    "# model = BlipForQuestionAnswering.from_pretrained(\"../../VITModel/blip-vqa-base/\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "# model = CLIPModel.from_pretrained(\"../../VITModel/clip-vit-base-patch32\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "# model = CLIPModel.from_pretrained(\"../../VITModel/clip-vit-large-patch14-336\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"../../VITModel/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"../../VITModel/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann paths ['./dataset/minigpt_casing_test/coco_caption/defe_ready_anno.json']\n",
      "{'caption': 'Defective.', 'image': '/ssd3/chih/Dataset/minigpt_casing_test/coco/image/test/casting512x512_deffront_0.jpeg', 'image_id': 'casting512x512_deffront_0.jpeg', 'instance_id': '0'}\n",
      "./dataset/minigpt_casing_test/coco/image/test/casting512x512_okfront_1169.jpeg \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.caption_datasets import COCOCaptionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# demo測試對話反饋\n",
    "model_config = {'amp':True, 'use_distributed':False,'accum_grad_iters':1,\n",
    "                'chat_template': True, 'end_sym': '\\n', 'prompt_template': \"<s>[INST] {} [/INST]\",\n",
    "                'max_txt_len': 1024, 'max_context_len': 3500,\n",
    "                'ouput_dir': './exper01_llama2',\n",
    "                'stage_ckpt': '/ssd3/chih/LLM/MiniGPT-4-ckpt/checkpoint_stage3.pth', \n",
    "                'vis_root_train': './dataset/minigpt_casing_train/coco/image/train',\n",
    "                'ann_paths_train': ['./dataset/minigpt_casing_train/coco_caption/defe_ready_anno.json'],\n",
    "                'vis_root_valid': './dataset/minigpt_casing_test/coco/image/test',\n",
    "                'ann_paths_valid': ['./dataset/minigpt_casing_test/coco_caption/defe_ready_anno.json']}\n",
    "train_data_set = COCOCaptionDataset(vis_root=model_config['vis_root_valid'], \n",
    "                                ann_paths=model_config['ann_paths_valid'],\n",
    "                                img_size=336)\n",
    "train_dataloader =DataLoader(train_data_set, batch_size=2, num_workers=10, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (forward_encoder): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, model_path = None, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.image_size = img_size\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.forward_encoder = CLIPModel.from_pretrained(model_path, torch_dtype=torch.float16).vision_model\n",
    "    def forward(self, x):\n",
    "        x = self.forward_encoder(x)\n",
    "        if len(x)!=1: #過濾掉clip中post層和自帶的layernorm\n",
    "            x = x[0]\n",
    "        return x\n",
    "model = VisionTransformer(model_path=\"../../VITModel/clip-vit-large-patch14\", img_size=336)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(\"/ssd3/chih/branch/VLM_Rebuild/dataset/minigpt_casing_train/coco/image/train/casting512x512_deffront_0.jpeg\")\n",
    "# inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "# image = Image.open(\"/ssd3/chih/branch/VLM_Rebuild/000000039769.jpg\")\n",
    "# inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "# inputs['pixel_values'].cuda().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_v2 = VisionTransformer(\n",
    "#     img_size=448,\n",
    "#     patch_size=14,\n",
    "#     use_mean_pooling=False,\n",
    "#     embed_dim=1408,\n",
    "#     depth=39,\n",
    "#     num_heads=1408//88,\n",
    "#     mlp_ratio=4.3637,\n",
    "#     qkv_bias=True,\n",
    "#     drop_path_rate=0,\n",
    "#     norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "#     # use_checkpoint=use_checkpoint,\n",
    "# )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2_output = model_v2(i['image'])\n",
    "# i['image'].cuda().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigptv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
